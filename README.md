# Topic-Modeling-on-News-Articles
#### LDA is a generative probability model, which means it attempts to provide a model for the distribution of outputs and inputs based on latent variables. This is opposed to discriminative models, which attempt to learn how inputs map to outputs.

![Schematic-of-LDA-algorithm](https://user-images.githubusercontent.com/98027899/172631337-af60f022-4229-4021-aa4f-a9a4e09758ba.png)


#### You can use LDA for a variety of tasks, from clustering customers based on product purchases to automatic harmonic analysis in music. However, it is most commonly associated with topic modeling in text corpuses. Observations are referred to as documents. The feature set is referred to as vocabulary. A feature is referred to as a word. And the resulting categories are referred to as topics.
#### An LDA model is defined by two parameters:

* α—A prior estimate on topic probability (in other words, the average frequency that each topic within a given document occurs).

* β—a collection of k topics where each topic is given a probability distribution over the vocabulary used in a document corpus, also called a "topic-word distribution."

####LDA is a "bag-of-words" model, which means that the order of words does not matter. LDA is a generative model where each document is generated word-by-word by choosing a topic mixture θ ∼ Dirichlet(α).

For each word in the document:

* Choose a topic z ∼ Multinomial(θ)

* Choose the corresponding topic-word distribution β_z.

* Draw a word w ∼ Multinomial(β_z).

* When training the model, the goal is to find parameters α and β, which maximize the probability that the text corpus is generated by the model.

The most popular methods for estimating the LDA model use Gibbs sampling or Expectation Maximization (EM) techniques. 

Results : Achieved coherence score of 0.6 for distributing 2200 BBC News documents into 10 major topics.
